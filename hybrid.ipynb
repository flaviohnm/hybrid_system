{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c96030f3be3e4a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T00:52:59.360703Z",
     "start_time": "2025-06-19T00:52:59.356446Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 1: IMPORTAÇÕES E SETUP GERAL\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9716172789275c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T00:52:59.414429Z",
     "start_time": "2025-06-19T00:52:59.376419Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.datasets import get_rdataset\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7acde5f0e5c4b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 2: FUNÇÕES AUXILIARES (SETUP E PROCESSAMENTO)\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ff04ae19676ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_seed(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68444ac84157ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_dataset(serie, dataset_name):\n",
    "    dir_path = \"./datasets\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    file_path = os.path.join(dir_path, f\"{dataset_name.lower()}.csv\")\n",
    "    df = pd.DataFrame({\"date\": serie.index, \"value\": serie.values})\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"-> Cópia do dataset '{dataset_name}' salva em: {file_path}\")\n",
    "\n",
    "def carregar_serie(nome):\n",
    "    print(f\"Buscando dados de '{nome}' via statsmodels...\")\n",
    "    nome_base = nome.lower()\n",
    "\n",
    "    if nome_base == \"airpassengers\":\n",
    "        df = get_rdataset(\"AirPassengers\", package=\"datasets\").data\n",
    "        serie = pd.Series(df['value'].values, index=pd.date_range(start=\"1949-01-01\", periods=len(df), freq=\"MS\"),\n",
    "                          name=\"AirPassengers\")\n",
    "    elif nome_base == \"lynx\":\n",
    "        df = get_rdataset(\"lynx\", package=\"datasets\").data\n",
    "        serie = pd.Series(df['value'].values, index=pd.date_range(start=\"1821\", periods=len(df), freq=\"A\"), name=\"Lynx\")\n",
    "    elif nome_base == \"co2\":\n",
    "        df = get_rdataset(\"CO2\", package=\"datasets\").data\n",
    "        df = df.ffill()\n",
    "        serie = pd.Series(df['value'].values, index=pd.date_range(start=\"1958-03-29\", periods=len(df), freq=\"MS\"),\n",
    "                          name=\"CO2\")\n",
    "    elif nome_base == \"sunspots\":\n",
    "        df = get_rdataset(\"sunspots\", package=\"datasets\").data\n",
    "        serie = pd.Series(df['value'].values, index=pd.date_range(start=\"1749-01-01\", periods=len(df), freq=\"MS\"),\n",
    "                          name=\"Sunspots\")\n",
    "    elif nome_base == \"austres\":\n",
    "        df = get_rdataset(\"austres\", package=\"datasets\").data\n",
    "        serie = pd.Series(df['value'].values, index=pd.date_range(start=\"1971-03-01\", periods=len(df), freq=\"QS-MAR\"),\n",
    "                          name=\"AustralianResidents\")\n",
    "    elif nome_base == \"nottem\":\n",
    "        df = get_rdataset(\"nottem\", package=\"datasets\").data\n",
    "        serie = pd.Series(df['value'].values, index=pd.date_range(start=\"1920-01-01\", periods=len(df), freq=\"MS\"),\n",
    "                          name=\"Nottingham\")\n",
    "    else:\n",
    "        raise ValueError(f\"Série '{nome}' não reconhecida.\")\n",
    "\n",
    "    salvar_dataset(serie, nome)\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a220cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_serie_temporal(serie, percentual_treino=0.7, percentual_validacao=0.15):\n",
    "    # ... (código da função inalterado) ...\n",
    "    tamanho_total = len(serie)\n",
    "    if tamanho_total < 20: \n",
    "        percentual_treino=0.8\n",
    "        percentual_validacao=0.0\n",
    "    ponto_corte_treino = int(tamanho_total * percentual_treino)\n",
    "    ponto_corte_validacao = int(tamanho_total * (percentual_treino + percentual_validacao))\n",
    "    treino = serie.iloc[:ponto_corte_treino]\n",
    "    validacao = serie.iloc[ponto_corte_treino:ponto_corte_validacao]\n",
    "    teste = serie.iloc[ponto_corte_validacao:]\n",
    "    return treino, validacao, teste\n",
    "\n",
    "def preparar_dados_para_neuralforecast(serie, nome_serie):\n",
    "    df = serie.reset_index()\n",
    "    df.columns = ['ds', 'y']\n",
    "    df['unique_id'] = nome_serie\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c38b31116385f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calcula o Mean Absolute Percentage Error (MAPE).\"\"\"\n",
    "    epsilon = 1e-10\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "def calculate_mase(y_true, y_pred, y_train):\n",
    "    \"\"\"Calcula o Mean Absolute Scaled Error (MASE).\"\"\"\n",
    "    n = len(y_train)\n",
    "    if n <= 1: return np.nan\n",
    "    d = np.sum(np.abs(y_train[1:] - y_train[:-1])) / (n - 1)\n",
    "    if d == 0: return np.inf\n",
    "    errors = np.mean(np.abs(y_true - y_pred))\n",
    "    return errors / d\n",
    "\n",
    "def calcular_metricas(y_true, y_pred, y_train):\n",
    "    \"\"\"Calcula um dicionário com todas as métricas de erro.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = calculate_mape(y_true, y_pred)\n",
    "    mase = calculate_mase(y_true, y_pred, y_train)\n",
    "    return {'RMSE': rmse, 'MAPE': mape, 'MASE': mase}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcc9d2b54e19eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 3: NOVAS FUNÇÕES PARA O MODELO HÍBRIDO HYS-MF\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "330a2a71e0b1c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_residuals_for_direct_forecast(residuals, h, input_size):\n",
    "    \"\"\"\n",
    "    Prepara os dados de resíduos para a abordagem de previsão direta.\n",
    "    Para cada horizonte h, cria um target específico.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(residuals) - input_size - h + 1):\n",
    "        X.append(residuals[i : i + input_size])\n",
    "        y.append(residuals[i + input_size : i + input_size + h])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Gera H dataframes, um para cada horizonte\n",
    "    dfs_por_horizonte = []\n",
    "    for i in range(h):\n",
    "        df = pd.DataFrame()\n",
    "        df['y'] = y[:, i] # Target é o resíduo do i-ésimo passo a frente\n",
    "        for j in range(input_size):\n",
    "            df[f'lag_{j+1}'] = X[:, input_size - 1 - j]\n",
    "        df['unique_id'] = 'residuos'\n",
    "        dfs_por_horizonte.append(df)\n",
    "        \n",
    "    return dfs_por_horizonte\n",
    "\n",
    "def treinar_e_prever_hys_mf(treino, validacao, teste, ordem_arima, seed, freq, input_size, h, max_steps=100):\n",
    "    \"\"\"\n",
    "    Implementa a metodologia HyS-MF do artigo.\n",
    "    \"\"\"\n",
    "    # 1. Treina o ARIMA e faz a previsão recursiva\n",
    "    treino_validacao_arima = pd.concat([treino, validacao]).asfreq(freq)\n",
    "    modelo_arima = ARIMA(treino_validacao_arima, order=ordem_arima).fit()\n",
    "    preds_arima = modelo_arima.predict(start=teste.index[0], end=teste.index[-1])\n",
    "    \n",
    "    # 2. Obtém os resíduos do ARIMA no conjunto de treino+validação\n",
    "    residuos = modelo_arima.resid\n",
    "    \n",
    "    # 3. Prepara os dados dos resíduos para a abordagem direta\n",
    "    # Para o N-BEATS, passamos o formato 'ds', 'y'\n",
    "    df_residuos_nf = preparar_dados_para_neuralforecast(residuos, \"residuos\")\n",
    "\n",
    "    # 4. Loop para treinar um modelo N-BEATS para cada horizonte\n",
    "    preds_residuos = []\n",
    "    for i in range(h):\n",
    "        # O N-BEATS da NeuralForecast já lida com lags, então podemos simplificar\n",
    "        # Treinamos um modelo para prever h passos e pegamos apenas o i-ésimo\n",
    "        modelos_residuos = [NBEATS(input_size=input_size, h=h, max_steps=max_steps, scaler_type='standard', random_seed=seed)]\n",
    "        nf_residuos = NeuralForecast(models=modelos_residuos, freq=freq)\n",
    "        nf_residuos.fit(df=df_residuos_nf)\n",
    "        \n",
    "        # Faz a previsão de h passos e seleciona apenas o que nos interessa\n",
    "        pred_h_passos = nf_residuos.predict()['NBEATS'].values\n",
    "        preds_residuos.append(pred_h_passos[i])\n",
    "\n",
    "    preds_residuos = np.array(preds_residuos)\n",
    "    \n",
    "    # 5. Previsão Híbrida = Previsão ARIMA + Previsão dos Resíduos\n",
    "    preds_hibrido = preds_arima.values + preds_residuos\n",
    "    \n",
    "    return preds_arima, preds_hibrido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b29a1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 4: EXECUÇÃO DO EXPERIMENTO COMPLETO\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd997c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar_experimento(nome_da_serie):\n",
    "    try:\n",
    "        N_CICLOS = 3 # Reduzido para um teste rápido devido à alta complexidade\n",
    "        ORDEM_ARIMA = (3, 1, 2)\n",
    "        SEED_INICIAL = 42\n",
    "\n",
    "        serie_completa = carregar_serie(nome_da_serie)\n",
    "        treino, validacao, teste = dividir_serie_temporal(serie_completa)\n",
    "        \n",
    "        if len(teste) < 2: return None\n",
    "        freq = serie_completa.index.freqstr or pd.infer_freq(serie_completa.index)\n",
    "        if freq is None: return None\n",
    "        \n",
    "        df_treino_validacao_nf = pd.concat([preparar_dados_para_neuralforecast(treino, nome_da_serie), \n",
    "                                            preparar_dados_para_neuralforecast(validacao, nome_da_serie)])\n",
    "        \n",
    "        h = len(teste)\n",
    "        input_size = 2 * h\n",
    "        resultados_finais = []\n",
    "\n",
    "        for i in tqdm(range(N_CICLOS), desc=f\"Ciclos para {nome_da_serie}\"):\n",
    "            definir_seed(SEED_INICIAL + i)\n",
    "            try:\n",
    "                # Modelo Híbrido (HyS-MF) e ARIMA\n",
    "                preds_arima, preds_hibrido = treinar_e_prever_hys_mf(treino, validacao, teste, ORDEM_ARIMA, SEED_INICIAL + i, freq, input_size, h, max_steps=50)\n",
    "                metricas_hibrido = calcular_metricas(teste.values, preds_hibrido, treino.values)\n",
    "                metricas_hibrido['modelo'] = 'Híbrido (HyS-MF)'\n",
    "                resultados_finais.append(metricas_hibrido)\n",
    "                \n",
    "                metricas_arima = calcular_metricas(teste.values, preds_arima.values, treino.values)\n",
    "                metricas_arima['modelo'] = 'ARIMA'\n",
    "                resultados_finais.append(metricas_arima)\n",
    "\n",
    "                # Modelo N-BEATS (na série original)\n",
    "                modelos_nbeats = [NBEATS(input_size=input_size, h=h, max_steps=50, scaler_type='standard', random_seed=SEED_INICIAL + i)]\n",
    "                nf_nbeats = NeuralForecast(models=modelos_nbeats, freq=freq)\n",
    "                nf_nbeats.fit(df=df_treino_validacao_nf)\n",
    "                preds_nbeats = nf_nbeats.predict()['NBEATS'].values\n",
    "                metricas_nbeats = calcular_metricas(teste.values, preds_nbeats, treino.values)\n",
    "                metricas_nbeats['modelo'] = 'N-BEATS'\n",
    "                resultados_finais.append(metricas_nbeats)\n",
    "            except Exception as e:\n",
    "                print(f\"AVISO: Ciclo {i+1} falhou. Erro: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not resultados_finais: return None\n",
    "        \n",
    "        df_resultados = pd.DataFrame(resultados_finais)\n",
    "        df_sumario = df_resultados.groupby('modelo').agg(['mean', 'std'])\n",
    "        return df_sumario.reindex(columns=['RMSE', 'MAPE', 'MASE'], level=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERRO GERAL no dataset '{nome_da_serie}': {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecd5da6e94b795c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 5: ORQUESTRADOR E RELATÓRIO FINAL\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a7481bae3f1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando dados de 'AirPassengers' via statsmodels...\n",
      "-> Cópia do dataset 'AirPassengers' salva em: ./datasets\\airpassengers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ciclos para AirPassengers:   0%|          | 0/3 [00:00<?, ?it/s]Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type          | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | loss         | MAE           | 0      | train\n",
      "1 | padder_train | ConstantPad1d | 0      | train\n",
      "2 | scaler       | TemporalNorm  | 0      | train\n",
      "3 | blocks       | ModuleList    | 2.5 M  | train\n",
      "-------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "3.0 K     Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.064    Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s, v_num=122, train_loss_step=0.0741, train_loss_epoch=0.0741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s, v_num=122, train_loss_step=0.0741, train_loss_epoch=0.0741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 90.88it/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type          | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | loss         | MAE           | 0      | train\n",
      "1 | padder_train | ConstantPad1d | 0      | train\n",
      "2 | scaler       | TemporalNorm  | 0      | train\n",
      "3 | blocks       | ModuleList    | 2.5 M  | train\n",
      "-------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "3.0 K     Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.064    Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s, v_num=124, train_loss_step=0.0741, train_loss_epoch=0.0741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s, v_num=124, train_loss_step=0.0741, train_loss_epoch=0.0741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 95.85it/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type          | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | loss         | MAE           | 0      | train\n",
      "1 | padder_train | ConstantPad1d | 0      | train\n",
      "2 | scaler       | TemporalNorm  | 0      | train\n",
      "3 | blocks       | ModuleList    | 2.5 M  | train\n",
      "-------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "3.0 K     Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.064    Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s, v_num=126, train_loss_step=0.0741, train_loss_epoch=0.0741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s, v_num=126, train_loss_step=0.0741, train_loss_epoch=0.0741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 242.53it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type          | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | loss         | MAE           | 0      | train\n",
      "1 | padder_train | ConstantPad1d | 0      | train\n",
      "2 | scaler       | TemporalNorm  | 0      | train\n",
      "3 | blocks       | ModuleList    | 2.5 M  | train\n",
      "-------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "3.0 K     Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.064    Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s, v_num=128, train_loss_step=0.0741, train_loss_epoch=0.0741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  3.23it/s, v_num=128, train_loss_step=0.0741, train_loss_epoch=0.0741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 100.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type          | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | loss         | MAE           | 0      | train\n",
      "1 | padder_train | ConstantPad1d | 0      | train\n",
      "2 | scaler       | TemporalNorm  | 0      | train\n",
      "3 | blocks       | ModuleList    | 2.5 M  | train\n",
      "-------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "3.0 K     Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.064    Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=130, train_loss_step=0.266, train_loss_epoch=0.266]        "
     ]
    }
   ],
   "source": [
    "LISTA_DE_DATASETS = ['AirPassengers'] # Começando com um para teste\n",
    "resultados_gerais = {}\n",
    "\n",
    "for dataset in LISTA_DE_DATASETS:\n",
    "    sumario = executar_experimento(dataset)\n",
    "    if sumario is not None:\n",
    "        resultados_gerais[dataset] = sumario\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"     RELATÓRIO FINAL: COMPARAÇÃO DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if resultados_gerais:\n",
    "    # Exibindo o resultado para o primeiro dataset da lista como exemplo\n",
    "    primeiro_dataset = LISTA_DE_DATASETS[0]\n",
    "    df_sumario_exemplo = resultados_gerais.get(primeiro_dataset)\n",
    "    if df_sumario_exemplo is not None:\n",
    "        print(f\"Resultados para o dataset: {primeiro_dataset}\")\n",
    "        # Reutilizando a função de exibição que já tínhamos\n",
    "        # exibir_sumario_estilizado(df_sumario_exemplo) # Se tiver essa função definida\n",
    "        display(df_sumario_exemplo.style.format('{:.3f}'))\n",
    "else:\n",
    "    print(\"Nenhum experimento foi concluído com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
