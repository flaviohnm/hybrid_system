{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ae555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 1: IMPORTAÇÕES E SETUP GERAL\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "# --- Nossas funções auxiliares agora são importadas! ---\n",
    "from helper.utils import (\n",
    "    definir_seed,\n",
    "    carregar_serie,\n",
    "    dividir_serie_temporal,\n",
    "    preparar_dados_para_neuralforecast\n",
    ")\n",
    "\n",
    "# --- Importações específicas para o treinamento ---\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pmdarima as pm\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS, MLP, LSTM, Autoformer, NHITS\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 2: PIPELINE DE EXPERIMENTO\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f152a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_melhor_arima_auto(treino_log, freq):\n",
    "    # (Mantenha sua função original aqui)\n",
    "    print(\"Buscando melhor ordem ARIMA com auto_arima...\")\n",
    "    m = 12 if freq.startswith('M') else (4 if freq.startswith('Q') else 1)\n",
    "    auto_arima_model = pm.auto_arima(treino_log, m=m, seasonal=True, trace=False,\n",
    "                                     error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "    print(\n",
    "        f\"Melhor ordem encontrada: {auto_arima_model.order} Sazonal: {auto_arima_model.seasonal_order}\")\n",
    "    return auto_arima_model.order, auto_arima_model.seasonal_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e125c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def executar_experimento(nome_da_serie, horizonte):\n",
    "    \"\"\"\n",
    "    Versão final e COMPLETA do pipeline, incluindo:\n",
    "    1. Modelo ARIMA.\n",
    "    2. Modelos Neurais Puros (não-híbridos).\n",
    "    3. Múltiplos Sistemas Híbridos (MIMO e Recursive).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Configurações Iniciais ---\n",
    "        SEED = 42\n",
    "        definir_seed(SEED)\n",
    "        INPUT_SIZE_NEURAL = 50 \n",
    "        MAX_STEPS_NEURAL = 200 \n",
    "        \n",
    "        # --- Carga e Preparação ---\n",
    "        serie_original = carregar_serie(nome_da_serie)\n",
    "        percentual_treino = 1 - (horizonte / len(serie_original))\n",
    "        \n",
    "        tamanho_treino_necessario = INPUT_SIZE_NEURAL + horizonte\n",
    "        if len(serie_original) * percentual_treino < tamanho_treino_necessario:\n",
    "            print(f\"AVISO: A série '{nome_da_serie}' (treino de {int(len(serie_original) * percentual_treino)} pts) é muito curta para o input_size={INPUT_SIZE_NEURAL} e horizonte={horizonte}. Pulando.\")\n",
    "            return None\n",
    "\n",
    "        treino_orig, teste_orig = dividir_serie_temporal(serie_original, percentual_treino=percentual_treino)\n",
    "        serie_log = np.log(serie_original)\n",
    "        treino_log, _ = dividir_serie_temporal(serie_log, percentual_treino=percentual_treino)\n",
    "        freq = serie_original.index.freqstr or pd.infer_freq(serie_original.index)\n",
    "        if freq is None: return None\n",
    "        previsoes_teste = {'y_true': teste_orig.values}\n",
    "        \n",
    "        # --- 1. Modelo ARIMA ---\n",
    "        modelo_arima = None\n",
    "        try:\n",
    "            print(f\"Processando: ARIMA para '{nome_da_serie}'\")\n",
    "            ordem, ordem_sazonal = encontrar_melhor_arima_auto(treino_log, freq)\n",
    "            modelo_arima = ARIMA(treino_log.asfreq(freq), order=ordem, seasonal_order=ordem_sazonal).fit()\n",
    "            preds_log_teste_arima = modelo_arima.forecast(steps=horizonte)\n",
    "            previsoes_teste['ARIMA'] = np.exp(preds_log_teste_arima).values\n",
    "        except Exception as e: \n",
    "            print(f\"AVISO: ARIMA falhou para '{nome_da_serie}': {e}\")\n",
    "            return None\n",
    "\n",
    "        # ==============================================================================\n",
    "        # --- 2. Modelos Neurais Puros (Não-Híbridos) ---\n",
    "        # ==============================================================================\n",
    "        print(\"\\n--- Processando Modelos Neurais Puros ---\")\n",
    "        df_treino_log_nf = preparar_dados_para_neuralforecast(treino_log, nome_da_serie)\n",
    "        modelos_para_testar = {\n",
    "            'N-BEATS': NBEATS, \n",
    "            'MLP': MLP, \n",
    "            'LSTM': LSTM, \n",
    "            'Autoformer': Autoformer, \n",
    "            'NHITS': NHITS\n",
    "        }\n",
    "        \n",
    "        for nome_modelo, classe_modelo in modelos_para_testar.items():\n",
    "            try:\n",
    "                print(f\"Processando: {nome_modelo} (Puro)\")\n",
    "                modelo_neural = [classe_modelo(input_size=INPUT_SIZE_NEURAL, h=horizonte, max_steps=MAX_STEPS_NEURAL, scaler_type='standard', random_seed=SEED)]\n",
    "                nf = NeuralForecast(models=modelo_neural, freq=freq)\n",
    "                nf.fit(df=df_treino_log_nf, verbose=False)\n",
    "                previsoes_teste[nome_modelo] = np.exp(nf.predict()[classe_modelo.__name__].values)\n",
    "            except Exception as e: \n",
    "                print(f\"AVISO: {nome_modelo} (Puro) falhou: {e}\")\n",
    "                previsoes_teste[nome_modelo] = np.nan\n",
    "        \n",
    "        # --- Preparação dos Resíduos para Modelos Híbridos ---\n",
    "        print(\"\\n--- Processando Modelos Híbridos ---\")\n",
    "        residuos_treino_log = modelo_arima.resid\n",
    "        df_residuos_treino = preparar_dados_para_neuralforecast(residuos_treino_log, \"residuos\")\n",
    "        \n",
    "        # --- 3. HÍBRIDO: ARIMA + N-BEATS (MIMO) ---\n",
    "        try:\n",
    "            print(\"Processando: Híbrido N-BEATS (MIMO)\")\n",
    "            modelo_nbeats_mimo = [NBEATS(input_size=INPUT_SIZE_NEURAL, h=horizonte, max_steps=MAX_STEPS_NEURAL, scaler_type='standard', random_seed=SEED)]\n",
    "            nf_nbeats_mimo = NeuralForecast(models=modelo_nbeats_mimo, freq=freq)\n",
    "            nf_nbeats_mimo.fit(df=df_residuos_treino, verbose=False)\n",
    "            preds_residuos_mimo = nf_nbeats_mimo.predict()['NBEATS'].values\n",
    "            previsoes_teste['Híbrido N-BEATS (MIMO)'] = previsoes_teste['ARIMA'] + preds_residuos_mimo\n",
    "        except Exception as e:\n",
    "            previsoes_teste['Híbrido N-BEATS (MIMO)'] = np.nan\n",
    "\n",
    "        # --- 4. HÍBRIDO: ARIMA + MLP (MIMO) ---\n",
    "        try:\n",
    "            print(\"Processando: Híbrido MLP (MIMO)\")\n",
    "            modelo_mlp_mimo = [MLP(input_size=INPUT_SIZE_NEURAL, h=horizonte, max_steps=MAX_STEPS_NEURAL, scaler_type='standard', random_seed=SEED)]\n",
    "            nf_mlp_mimo = NeuralForecast(models=modelo_mlp_mimo, freq=freq)\n",
    "            nf_mlp_mimo.fit(df=df_residuos_treino, verbose=False)\n",
    "            preds_residuos_mlp_mimo = nf_mlp_mimo.predict()['MLP'].values\n",
    "            previsoes_teste['Híbrido MLP (MIMO)'] = previsoes_teste['ARIMA'] + preds_residuos_mlp_mimo\n",
    "        except Exception as e:\n",
    "            previsoes_teste['Híbrido MLP (MIMO)'] = np.nan\n",
    "\n",
    "        # --- 5. HÍBRIDO: ARIMA + LSTM (MIMO) ---\n",
    "        try:\n",
    "            print(\"Processando: Híbrido LSTM (MIMO)\")\n",
    "            modelo_lstm_mimo = [LSTM(input_size=INPUT_SIZE_NEURAL, h=horizonte, max_steps=MAX_STEPS_NEURAL, scaler_type='standard', random_seed=SEED)]\n",
    "            nf_lstm_mimo = NeuralForecast(models=modelo_lstm_mimo, freq=freq)\n",
    "            nf_lstm_mimo.fit(df=df_residuos_treino, verbose=False)\n",
    "            preds_residuos_lstm_mimo = nf_lstm_mimo.predict()['LSTM'].values\n",
    "            previsoes_teste['Híbrido LSTM (MIMO)'] = previsoes_teste['ARIMA'] + preds_residuos_lstm_mimo\n",
    "        except Exception as e:\n",
    "            previsoes_teste['Híbrido LSTM (MIMO)'] = np.nan\n",
    "\n",
    "        # --- 6. HÍBRIDO: ARIMA + N-BEATS (Recursive-Direct) ---\n",
    "        try:\n",
    "            print(\"Processando: Híbrido N-BEATS (Recursive-Direct)\")\n",
    "            \n",
    "            # 1. Preparação dos dados em formato Painel\n",
    "            lista_dfs_horizontes = []\n",
    "            for h_step in range(1, horizonte + 1):\n",
    "                df_residuos_h = residuos_treino_log.to_frame(name='y')\n",
    "                df_residuos_h['ds'] = df_residuos_h.index\n",
    "                df_residuos_h['y'] = df_residuos_h['y'].shift(-h_step + 1)\n",
    "                df_residuos_h['unique_id'] = f'h_{h_step}'\n",
    "                df_residuos_h.dropna(inplace=True)\n",
    "                \n",
    "                if len(df_residuos_h) >= INPUT_SIZE_NEURAL + 1:\n",
    "                    lista_dfs_horizontes.append(df_residuos_h)\n",
    "\n",
    "            if not lista_dfs_horizontes:\n",
    "                 raise ValueError(\"Nenhum horizonte tinha dados suficientes para treinar o modelo 'Direct'.\")\n",
    "\n",
    "            df_treino_residuos_painel = pd.concat(lista_dfs_horizontes)\n",
    "\n",
    "            # 2. Treinamento e Previsão\n",
    "            modelo_direct = [NBEATS(input_size=INPUT_SIZE_NEURAL, h=1, max_steps=MAX_STEPS_NEURAL, scaler_type='standard', random_seed=SEED)]\n",
    "            nf_direct = NeuralForecast(models=modelo_direct, freq=freq)\n",
    "            nf_direct.fit(df=df_treino_residuos_painel, verbose=False)\n",
    "            preds_df = nf_direct.predict()\n",
    "            \n",
    "            # 3. Organização e Combinação dos Resultados\n",
    "            preds_df['h_step'] = preds_df['unique_id'].str.replace('h_', '').astype(int)\n",
    "            preds_df.sort_values(by='h_step', inplace=True)\n",
    "            preds_residuos_direct = preds_df['NBEATS'].values\n",
    "\n",
    "            if len(preds_residuos_direct) < horizonte:\n",
    "                preds_completas = np.zeros(horizonte)\n",
    "                indices_validos = preds_df['h_step'].values - 1\n",
    "                preds_completas[indices_validos] = preds_residuos_direct\n",
    "                preds_residuos_direct = preds_completas\n",
    "\n",
    "            previsoes_teste['Híbrido N-BEATS (Direct)'] = previsoes_teste['ARIMA'] + preds_residuos_direct\n",
    "\n",
    "        except Exception as e: \n",
    "            print(f\"AVISO: Híbrido N-BEATS (Direct) falhou: {e}\")\n",
    "            previsoes_teste['Híbrido N-BEATS (Direct)'] = np.nan\n",
    "            \n",
    "        # --- 7. HÍBRIDO: ARIMA + N-BEATS (Recursive) ---\n",
    "        try:\n",
    "            print(\"Processando: Híbrido N-BEATS (Recursive)\")\n",
    "            modelo_nbeats_rec = [NBEATS(input_size=INPUT_SIZE_NEURAL, h=1, max_steps=MAX_STEPS_NEURAL, scaler_type='standard', random_seed=SEED)]\n",
    "            nf_nbeats_rec = NeuralForecast(models=modelo_nbeats_rec, freq=freq)\n",
    "            nf_nbeats_rec.fit(df=df_residuos_treino, verbose=False)\n",
    "            preds_residuos_rec_df = nf_nbeats_rec.predict(h=horizonte)\n",
    "            preds_residuos_log_rec = preds_residuos_rec_df['NBEATS'].values\n",
    "            previsoes_teste['Híbrido N-BEATS (Recursive)'] = previsoes_teste['ARIMA'] + np.array(preds_residuos_log_rec)\n",
    "        except Exception as e: \n",
    "            previsoes_teste['Híbrido N-BEATS (Recursive)'] = np.nan\n",
    "\n",
    "        # --- Finalização ---\n",
    "        df_final = pd.DataFrame(previsoes_teste, index=teste_orig.index)\n",
    "        df_final['dataset'] = nome_da_serie\n",
    "        df_final['horizonte'] = horizonte\n",
    "        return df_final.reset_index().rename(columns={'index': 'ds'})\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"ERRO GERAL no processamento de '{nome_da_serie}' para o horizonte {horizonte}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 3: ORQUESTRADOR (LÓGICA DA CAMADA SILVER)\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurações ---\n",
    "LISTA_DE_DATASETS = ['AirPassengers', 'co2', 'nottem', 'JohnsonJohnson', 'UKgas']\n",
    "# LISTA_DE_DATASETS = ['AirPassengers']\n",
    "VETOR_DE_HORIZONTES = [10, 12, 15, 24]\n",
    "output_dir = \"./data/silver\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"resultados_completos.csv\")\n",
    "\n",
    "# --- Lógica de Execução Incremental ---\n",
    "# Apaga o arquivo antigo para garantir uma execução limpa\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "header_escrito = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=45, train_loss_step=0.0334, train_loss_epoch=0.0334]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type          | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | loss         | MAE           | 0      | train\n",
      "1 | padder_train | ConstantPad1d | 0      | train\n",
      "2 | scaler       | TemporalNorm  | 0      | train\n",
      "3 | mlp          | ModuleList    | 1.1 M  | train\n",
      "4 | out          | Linear        | 10.2 K | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.448     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVISO: N-BEATS (Puro) falhou: name 'exit' is not defined\n",
      "Processando: MLP (Puro)\n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=46, train_loss_step=0.0636, train_loss_epoch=0.0636]        "
     ]
    }
   ],
   "source": [
    "for dataset in tqdm(LISTA_DE_DATASETS, desc=\"Processando Datasets\"):\n",
    "    for horizonte in tqdm(VETOR_DE_HORIZONTES, desc=f\"Testando Horizontes para {dataset}\", leave=False):\n",
    "        # Executa o experimento para uma combinação de dataset e horizonte\n",
    "        df_resultado_detalhado = executar_experimento(dataset, horizonte)\n",
    "\n",
    "        if df_resultado_detalhado is not None:\n",
    "            # Lógica para salvar incrementalmente no arquivo da camada Silver\n",
    "            if not header_escrito:\n",
    "                # Escreve o cabeçalho apenas na primeira vez\n",
    "                df_resultado_detalhado.to_csv(\n",
    "                    output_file, index=False, mode='w', header=True)\n",
    "                header_escrito = True\n",
    "            else:\n",
    "                # Anexa os novos resultados sem o cabeçalho\n",
    "                df_resultado_detalhado.to_csv(\n",
    "                    output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"\\nProcesso finalizado. Resultados salvos em '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
