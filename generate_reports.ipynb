{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f929353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 1: IMPORTAÇÕES E SETUP GERAL (APENAS PARA ANÁLISE)\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm import tqdm\n",
    "from scikit_posthocs._critical_values import get_critical_value\n",
    "\n",
    "# --- Importando as funções compartilhadas necessárias ---\n",
    "from helper.utils import carregar_serie, dividir_serie_temporal, calcular_metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7db0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 2: FUNÇÕES DE GERAÇÃO DE RELATÓRIOS (CAMADA GOLD)\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas_finais(df_results):\n",
    "    \"\"\"\n",
    "    Função auxiliar robusta para calcular as métricas a partir do DataFrame de previsões brutas.\n",
    "    \"\"\"\n",
    "    modelos = [col for col in df_results.columns if col not in ['ds', 'y_true', 'dataset', 'horizonte']]\n",
    "    \n",
    "    # Dicionário para armazenar dados de treino para o cálculo do MASE\n",
    "    y_train_dict = {}\n",
    "    \n",
    "    # Para cada dataset, carrega os dados de treino e teste originais para o cálculo do MASE\n",
    "    for dataset_nome in df_results['dataset'].unique():\n",
    "        horizonte_max_ds = df_results[df_results['dataset'] == dataset_nome]['horizonte'].max()\n",
    "        serie_original = carregar_serie(dataset_nome)\n",
    "        percentual_treino = 1 - (horizonte_max_ds / len(serie_original))\n",
    "        treino, _ = dividir_serie_temporal(serie_original, percentual_treino)\n",
    "        y_train_dict[dataset_nome] = treino.values\n",
    "\n",
    "    # Transforma o DataFrame de 'largo' para 'longo' para facilitar a agregação\n",
    "    df_melted = df_results.melt(id_vars=['ds', 'y_true', 'dataset', 'horizonte'], \n",
    "                                value_vars=modelos, var_name='Modelo', value_name='y_pred')\n",
    "    \n",
    "    metricas_gerais = []\n",
    "    # Agrupa para calcular as métricas para cada combinação de dataset, horizonte e modelo\n",
    "    for (dataset, horizonte, modelo), group in tqdm(df_melted.groupby(['dataset', 'horizonte', 'Modelo']), desc=\"Calculando Métricas\"):\n",
    "        if not group['y_pred'].isnull().all():\n",
    "            # Passa a fatia correta dos dados de treino para o MASE\n",
    "            y_train_correto = y_train_dict[dataset]\n",
    "            \n",
    "            # Calcula as métricas\n",
    "            metricas_dict = calcular_metricas(group['y_true'], group['y_pred'], y_train_correto)\n",
    "            \n",
    "            # Adiciona as informações de metadados ao dicionário\n",
    "            metricas_dict['dataset'] = dataset\n",
    "            metricas_dict['horizonte'] = horizonte\n",
    "            metricas_dict['Modelo'] = modelo\n",
    "            \n",
    "            metricas_gerais.append(metricas_dict)\n",
    "    \n",
    "    df_metricas_final = pd.DataFrame(metricas_gerais)\n",
    "    # Renomeia colunas para clareza nos relatórios\n",
    "    df_metricas_final.rename(columns={'RMSE': 'Mean RMSE', 'MAPE(%)': 'Mean MAPE(%)', 'MASE': 'Mean MASE'}, inplace=True)\n",
    "    return df_metricas_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_evolucao_erro(df_metricas, vetor_horizontes):\n",
    "    \"\"\"RELATÓRIO 1: Gera o gráfico de linha da evolução do erro.\"\"\"\n",
    "    print(\"\\n--- RELATÓRIO 1: EVOLUÇÃO DO ERRO (RMSE) POR HORIZONTE ---\")\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.lineplot(data=df_metricas, x='horizonte', y='Mean RMSE', hue='Modelo', style='Modelo', markers=True, dashes=False)\n",
    "    plt.title(\"Evolução do Erro (RMSE) com o Aumento do Horizonte\", fontsize=16)\n",
    "    plt.xlabel(\"Horizonte de Previsão\"); plt.ylabel(\"RMSE Médio\"); plt.grid(True)\n",
    "    if not df_metricas.empty:\n",
    "        plt.xticks(vetor_horizontes)\n",
    "    plt.legend(title='Modelo'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_desempenho_agregado(df_foco):\n",
    "    \"\"\"RELATÓRIO 2: Mostra a tabela de desempenho geral agregado.\"\"\"\n",
    "    print(\"\\n--- RELATÓRIO 2: DESEMPENHO GERAL (MÉDIA NO HORIZONTE MAIS LONGO) ---\")\n",
    "    df_agrupado = df_foco.groupby('Modelo')[['Mean RMSE', 'Mean MAPE(%)', 'Mean MASE']].mean()\n",
    "    display(df_agrupado.style.format('{:.3f}').highlight_min(axis=0, props='background-color: #4285F4; color: white;'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914be97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_desempenho_detalhado(df_foco):\n",
    "    \"\"\"RELATÓRIO 3: Mostra a tabela de desempenho detalhado por dataset.\"\"\"\n",
    "    print(\"\\n--- RELATÓRIO 3: DESEMPENHO DETALHADO POR DATASET (HORIZONTE MAIS LONGO) ---\")\n",
    "    df_reporte_detalhado = df_foco.set_index(['dataset', 'Modelo']).drop(columns=['horizonte'])\n",
    "    display(df_reporte_detalhado.style.format('{:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdfed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_tabela_ranking(df_foco):\n",
    "    \"\"\"RELATÓRIO 4: Mostra a tabela de ranking e a retorna para uso posterior.\"\"\"\n",
    "    print(\"\\n--- RELATÓRIO 4: RANKING DOS MODELOS (BASEADO EM RMSE, HORIZONTE MAIS LONGO) ---\")\n",
    "    df_rank = df_foco.copy()\n",
    "    df_rank['Rank'] = df_rank.groupby('dataset')['Mean RMSE'].rank().astype(int)\n",
    "    df_pivot_rank = df_rank.pivot_table(index='dataset', columns='Modelo', values='Rank')\n",
    "    if len(df_pivot_rank) > 1:\n",
    "        df_pivot_rank.loc['Média do Rank'] = df_pivot_rank.mean(axis=0)\n",
    "    display(df_pivot_rank.style.format('{:.1f}').highlight_min(axis=1, props='background-color: #4285F4; color: white;'))\n",
    "    return df_pivot_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_diferenca_percentual(df_foco):\n",
    "    \"\"\"RELATÓRIO 5: Mostra o gráfico de ganho percentual do modelo híbrido.\"\"\"\n",
    "    print(\"\\n--- RELATÓRIO 5: GANHO PERCENTUAL DO MODELO HÍBRIDO (BASEADO EM MAPE, HORIZONTE MAIS LONGO) ---\")\n",
    "    df_pivot_mape = df_foco.pivot_table(index='dataset', columns='Modelo', values='Mean MAPE(%)')\n",
    "    modelo_referencia_hibrido = 'Híbrido (MIMO)'\n",
    "    if modelo_referencia_hibrido in df_pivot_mape.columns:\n",
    "        mape_hibrido = df_pivot_mape[modelo_referencia_hibrido]\n",
    "        df_pd = pd.DataFrame(index=df_pivot_mape.index)\n",
    "        for modelo in [m for m in df_pivot_mape.columns if m != modelo_referencia_hibrido]:\n",
    "            df_pd[f'Ganho sobre {modelo} (%)'] = 100 * (df_pivot_mape[modelo] - mape_hibrido) / df_pivot_mape[modelo]\n",
    "        ax = df_pd.plot(kind='bar', figsize=(14, 7), grid=True, rot=45); ax.set_ylabel(\"Melhora Percentual (%)\"); ax.set_xlabel(\"Dataset\")\n",
    "        ax.set_title(f\"Diferença Percentual (PD%): Ganho de Performance do {modelo_referencia_hibrido}\"); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_analise_estatistica_demsar(df_pivot_rank, maior_horizonte):\n",
    "    \"\"\"RELATÓRIO 6: Executa e exibe os testes de Friedman e Nemenyi.\"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*60); print(f\"     RELATÓRIO 6: ANÁLISE ESTATÍSTICA GLOBAL (FRIEDMAN + NEMENYI, HORIZONTE {int(maior_horizonte)})\"); print(\"=\"*60)\n",
    "    \n",
    "    # --- CORREÇÃO APLICADA AQUI ---\n",
    "    # Inicializamos as variáveis como None para garantir que sempre existam.\n",
    "    p_values_nemenyi = None\n",
    "    avg_ranks = None\n",
    "    \n",
    "    df_rank_data = df_pivot_rank.drop('Média do Rank', errors='ignore')\n",
    "    if df_rank_data.empty:\n",
    "        print(\"AVISO: Tabela de ranking vazia, não foi possível executar a análise estatística.\")\n",
    "        # Retorna os valores nulos\n",
    "        return p_values_nemenyi, avg_ranks\n",
    "\n",
    "    try:\n",
    "        stat, p_value = friedmanchisquare(*[df_rank_data[col].values for col in df_rank_data.columns])\n",
    "        print(f\"\\n--- Teste de Friedman ---\\np-valor: {p_value:.4f}\")\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            print(\"\\n**Conclusão: Há uma diferença estatisticamente significativa entre os modelos.**\")\n",
    "            \n",
    "            print(\"\\n--- Teste Post-hoc de Nemenyi (p-valores par a par) ---\")\n",
    "            df_rank_melted = df_rank_data.reset_index().melt(id_vars='dataset', var_name='Modelo', value_name='Rank')\n",
    "            p_values_nemenyi = sp.posthoc_nemenyi_friedman(df_rank_melted, melted=True, group_col='Modelo', block_col='dataset', y_col='Rank')\n",
    "            display(p_values_nemenyi.style.format('{:.3f}').applymap(lambda x: 'background-color: lightgreen' if x < 0.05 else ''))\n",
    "\n",
    "            # Calcula os ranks médios apenas se o teste for significativo\n",
    "            avg_ranks = df_pivot_rank.mean(axis=0).drop('Média do Rank', errors='ignore')\n",
    "        else:\n",
    "            print(\"\\n**Conclusão: Não há evidência de uma diferença estatística significativa entre os modelos.**\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"AVISO: A análise estatística avançada falhou: {e}\")\n",
    "        \n",
    "    # A função agora sempre retorna uma tupla, mesmo que os valores sejam None\n",
    "    return p_values_nemenyi, avg_ranks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_diagrama_diferenca_critica(df_pivot_rank, alpha=0.05):\n",
    "    \"\"\"RELATÓRIO ADICIONAL: Gera o Diagrama de Diferença Crítica (CD Plot).\"\"\"\n",
    "    if df_pivot_rank.empty:\n",
    "        print(\"AVISO: Tabela de ranking vazia, não é possível gerar o CD Plot.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\\\n--- DIAGRAMA DE DIFERENÇA CRÍTICA (CD PLOT) ---\")\n",
    "    display(Markdown(\"Este gráfico resume o teste de Nemenyi. Modelos que **NÃO** são significativamente diferentes estão conectados por uma linha horizontal.\"))\n",
    "    \n",
    "    # Extrai os dados de ranking e os nomes dos modelos\n",
    "    rank_data = df_pivot_rank.drop('Média do Rank', errors='ignore').values\n",
    "    model_names = df_pivot_rank.columns\n",
    "    \n",
    "    # Calcula a diferença crítica (CD)\n",
    "    # N = número de datasets, k = número de modelos\n",
    "    N = len(rank_data)\n",
    "    k = len(model_names)\n",
    "    \n",
    "    # Importa a tabela de valores críticos q_alpha do scikit-posthocs\n",
    "   \n",
    "    q_alpha = get_critical_value(k, alpha)\n",
    "    \n",
    "    cd = q_alpha * np.sqrt(k * (k + 1) / (6 * N))\n",
    "    \n",
    "    # Calcula os ranks médios\n",
    "    avg_ranks = df_pivot_rank.loc['Média do Rank'].sort_values()\n",
    "    \n",
    "    # Gera o gráfico usando a função de plotagem do scikit-posthocs\n",
    "    sp.sign_plot(avg_ranks, cd=cd)\n",
    "    plt.title(f\"Diagrama de Diferença Crítica (Teste de Nemenyi, alpha={alpha})\", fontsize=16)\n",
    "    plt.xlabel(\"Ranking Médio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NOVA FUNÇÃO DE RELATÓRIO (BASEADA NA TABELA 4 DO ARTIGO) ---\n",
    "def exibir_tabela_ranking(df_foco, metrica='Mean RMSE'):\n",
    "    \"\"\"RELATÓRIO 4: Mostra a tabela de ranking dos modelos.\"\"\"\n",
    "    print(f\"\\n--- RELATÓRIO 4: RANKING DOS MODELOS (BASEADO EM {metrica}) ---\")\n",
    "    df_rank = df_foco.copy()\n",
    "    rank_col_name = f'Rank_{metrica}'\n",
    "    df_rank[rank_col_name] = df_rank.groupby('dataset')[metrica].rank().astype(int)\n",
    "    df_pivot_rank = df_rank.pivot_table(index='dataset', columns='Modelo', values=rank_col_name)\n",
    "    if len(df_pivot_rank) > 1:\n",
    "        df_pivot_rank.loc['Média do Rank'] = df_pivot_rank.mean(axis=0)\n",
    "    display(df_pivot_rank.style.format('{:.1f}').highlight_min(axis=1, props='background-color: #4285F4; color: white;'))\n",
    "    return df_pivot_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d62931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_ranking_mape_artigo(df_foco):\n",
    "    \"\"\"RELATÓRIO EXTRA 1: Gera a tabela de ranking por MAPE, no estilo da Tabela 4 do artigo.\"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"     RELATÓRIO ADICIONAL: TABELA DE RANKING POR MAPE (ESTILO ARTIGO)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_rank_mape = df_foco.copy()\n",
    "    df_rank_mape['Rank_MAPE'] = df_rank_mape.groupby('dataset')['Mean MAPE(%)'].rank().astype(int)\n",
    "    df_pivot = df_rank_mape.pivot_table(index='Modelo', columns='dataset', values='Rank_MAPE')\n",
    "    \n",
    "    # Calcula a Média e Mediana do Rank para cada modelo\n",
    "    df_pivot['Média'] = df_pivot.mean(axis=1)\n",
    "    df_pivot['Mediana'] = df_pivot.median(axis=1)\n",
    "    \n",
    "    def highlight_top3(s):\n",
    "        is_top3 = s <= 3\n",
    "        return ['background-color: blue' if v else '' for v in is_top3]\n",
    "\n",
    "    styled_pivot = (df_pivot.style\n",
    "                    .format('{:.1f}')\n",
    "                    .apply(highlight_top3, subset=pd.IndexSlice[:, [c for c in df_pivot.columns if c not in ['Média', 'Mediana']]])\n",
    "                    .set_caption(\"Ranking dos modelos por dataset baseado no MAPE (1 = Melhor).\"))\n",
    "    \n",
    "    display(styled_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4841e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NOVA FUNÇÃO DE RELATÓRIO (BASEADA NA FIGURA 9 DO ARTIGO) ---\n",
    "def plotar_pd_agregado(df_foco):\n",
    "    \"\"\"RELATÓRIO EXTRA 2: Gera o gráfico de Diferença Percentual (PD%) agregado.\"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"     RELATÓRIO ADICIONAL: GRÁFICO DE DIFERENÇA PERCENTUAL AGREGADO (ESTILO FIGURA 9)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    modelo_referencia_hibrido = 'Híbrido (MIMO)'\n",
    "    # Calcula o MAPE médio para cada modelo, em todos os datasets\n",
    "    df_mean_mape = df_foco.groupby('Modelo')['Mean MAPE(%)'].mean()\n",
    "\n",
    "    if modelo_referencia_hibrido in df_mean_mape.index:\n",
    "        mape_hibrido = df_mean_mape[modelo_referencia_hibrido]\n",
    "        pd_values = []\n",
    "        for modelo, mape_medio in df_mean_mape.items():\n",
    "            if modelo != modelo_referencia_hibrido:\n",
    "                pd_value = 100 * (mape_medio - mape_hibrido) / mape_medio\n",
    "                pd_values.append({'Modelo': modelo, 'PD(%)': pd_value})\n",
    "        \n",
    "        if pd_values:\n",
    "            df_pd = pd.DataFrame(pd_values).sort_values(by='PD(%)')\n",
    "            \n",
    "            plt.figure(figsize=(14, 8))\n",
    "            ax = sns.barplot(x='Modelo', y='PD(%)', data=df_pd, palette='viridis', hue='Modelo', legend=False)\n",
    "            ax.set_title(f\"Diferença Percentual (PD%) Agregada do {modelo_referencia_hibrido}\", fontsize=16)\n",
    "            ax.set_ylabel(\"Melhora Percentual (%)\")\n",
    "            ax.set_xlabel(\"Modelo Competidor\")\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c970d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 3: FUNÇÕES DE GERAÇÃO DE RELATÓRIOS (CAMADA GOLD)\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_suite_completa_de_relatorios(output_file, vetor_horizontes):\n",
    "    \"\"\"Função principal que orquestra a geração de todos os relatórios a partir da camada Silver.\"\"\"\n",
    "    print(\"=\"*60); print(\"     INICIANDO GERAÇÃO DA SUÍTE COMPLETA DE RELATÓRIOS (CAMADA GOLD)\"); print(\"=\"*60)\n",
    "    try:\n",
    "        # 1. Carrega os dados da camada Silver\n",
    "        df_results = pd.read_csv(output_file)\n",
    "        \n",
    "        # 2. Processa as métricas\n",
    "        df_metricas_final = calcular_metricas_finais(df_results)\n",
    "        \n",
    "        # 3. Gera os relatórios\n",
    "        plotar_evolucao_erro(df_metricas_final, vetor_horizontes)\n",
    "        \n",
    "        maior_horizonte = df_metricas_final['horizonte'].max()\n",
    "        df_foco_maior_h = df_metricas_final[df_metricas_final['horizonte'] == maior_horizonte]\n",
    "        \n",
    "        display(Markdown(f\"### Análises Detalhadas para o Horizonte Mais Longo ({int(maior_horizonte)} passos)\"))\n",
    "        \n",
    "        exibir_desempenho_agregado(df_foco_maior_h)\n",
    "        exibir_desempenho_detalhado(df_foco_maior_h)\n",
    "        df_pivot_rank = exibir_tabela_ranking(df_foco_maior_h)\n",
    "        \n",
    "        # Análise estatística\n",
    "        p_values_nemenyi, avg_ranks = exibir_analise_estatistica_demsar(df_pivot_rank, maior_horizonte)\n",
    "        plotar_diagrama_diferenca_critica(p_values_nemenyi, avg_ranks) # Adapte para usar os outputs corretos\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60); print(\"     SUÍTE DE RELATÓRIOS GERADA COM SUCESSO!\"); print(\"=\"*60)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\\\nERRO: Arquivo da camada Silver '{output_file}' não encontrado. Execute primeiro o script '01_run_experiments'.\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Ocorreu um erro ao gerar os relatórios: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SEÇÃO 4: EXECUÇÃO DA CAMADA GOLD\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84693533",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_layer_file = \"./data/silver/resultados_completos.csv\"\n",
    "VETOR_DE_HORIZONTES = [10, 12, 15, 24]\n",
    "\n",
    "gerar_suite_completa_de_relatorios(silver_layer_file, VETOR_DE_HORIZONTES)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
